{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project: Design and implement an LLM powered chatbot. This chatbot will be able to have a conversation and remember interaction history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GROQ API Key is: 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\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os  # Import the os module for environment variable handling\n",
    "from dotenv import load_dotenv  # Import dotenv to load environment variables from .env files\n",
    "\n",
    "load_dotenv()  # Load the environment variables from .env files\n",
    "\n",
    "# Retrieve the Groq API key from environment variables\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY not found. Check your .env file.\") # Raise an error if the key is not found\n",
    "else:\n",
    "    print(f\"The GROQ API Key is: {groq_api_key}\") # Print the API key for confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001B48819F150>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B48819C390>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Initialization\n",
    "from langchain_groq import ChatGroq # Import the ChatGroq class from langchain_groq\n",
    "\n",
    "# Initialize the chatbot model using the specified model name and API key\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Dahab, it's nice to meet you! That's impressive, a Senior AI Engineer! \\n\\nWhat kind of projects are you working on these days?  I'm always interested in learning more about what people are doing in the field of AI.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 23, 'total_tokens': 81, 'completion_time': 0.105454545, 'prompt_time': 0.00013801, 'queue_time': 0.018804977, 'total_time': 0.105592555}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-766becd0-4e10-4011-87a9-1ffe1cc503c9-0', usage_metadata={'input_tokens': 23, 'output_tokens': 58, 'total_tokens': 81})"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single Turn Interaction\n",
    "from langchain_core.messages import HumanMessage # Import HumanMessage for user input\n",
    "\n",
    "# Send a human message to the model to initiate a conversation\n",
    "model.invoke([HumanMessage(content=\"Hi, My name is Dahab and I am a Senior AI Engineer\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Dahab, and you are a Senior AI Engineer. \\n\\nIs there anything else you'd like to tell me about yourself or your work? üòä  I'm curious to learn more! \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 112, 'total_tokens': 159, 'completion_time': 0.085454545, 'prompt_time': 0.003870707, 'queue_time': 0.02026097, 'total_time': 0.089325252}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-5fbe82bd-732d-4089-9c45-8f09e810f2a5-0', usage_metadata={'input_tokens': 112, 'output_tokens': 47, 'total_tokens': 159})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-turn Interaction\n",
    "from langchain_core.messages import AIMessage # Import AIMessage for responses from the model\n",
    "\n",
    "# Send a series of messages including a conversation history\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, My name is Dahab and I am a Senior AI Engineer\"),\n",
    "        AIMessage(content=\"Hello Dahab, it's nice to meet you! \\n\\nAs a Senior AI Engineer yourself, I'm sure you have a lot of experience and knowledge to share. What kind of AI projects are you currently working on?  \\n\\nI'm always eager to learn more about the latest advancements in the field.\\n\"),\n",
    "        HumanMessage(content=\"Hi, what's my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History\n",
    "Use Message History to keep track of inputs and outputs of the model, store them in a datastore. Future interactions will load these messages and pass them into the chain as part of the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session Management\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory  # Import history for chat sessions\n",
    "from langchain_core.chat_history import BaseChatMessageHistory  # Base class for chat histories\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory  # To handle sessions with message history\n",
    "\n",
    "## Function to create a session ID for each chat\n",
    "store={} # Dictionary to store chat session history\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory: \n",
    "    if session_id not in store: # Check if session history exists\n",
    "        store[session_id] = ChatMessageHistory() # Create new history if not present\n",
    "    return store[session_id] # Return the chat session history for the session ID\n",
    "\n",
    "# Create a runnable model instance that maintains message history\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First chat session with session ID 'chat1'\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}} # Configuration dictionary for session identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model within the first session\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, My name is Dahab and I am a Senior AI Engineer\")], # User prompt\n",
    "    config=config # Use session configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Dahab, it's nice to meet you!\\n\\nAs a Senior AI Engineer, I imagine you have a lot of interesting projects and challenges you work on. What are some of the things you're most passionate about in the field of AI?\\n\""
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content # Output the AI's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Dahab.  You told me at the beginning of our conversation! üòä \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 91, 'total_tokens': 124, 'completion_time': 0.06, 'prompt_time': 0.002592481, 'queue_time': 0.022756358, 'total_time': 0.062592481}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-6afd5250-7e69-4161-84a1-038ec3bb907e-0', usage_metadata={'input_tokens': 91, 'output_tokens': 33, 'total_tokens': 124})"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Follow-up question to check the AI's knowledge of user identity\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I do not have access to any personal information about you, including your name. If you'd like to tell me your name, I'd be happy to know!\\n\""
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the session ID configuration for a new chat session\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")], # User question in new session\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, John! It's nice to meet you.  Is there anything I can help you with today?\\n\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User provides a name in the new session\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is John.\")], # User provides a name\n",
    "    config=config1 # Use the new session configuration\n",
    ")\n",
    "response.content # Output the AI's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is John. üòä \\n\\nI remember!  \\n\\nIs there anything else I can help you with, John?\\n'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check AI's understanding of the new user's name\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")], # Follow-up question\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Tempalate: Turn raw user information into LLM usable format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Setup for Basic Interaction\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Creating a chat prompt template with a system message and message placeholders for dynamic content\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Remember user details and respond accordingly.\", # System instruction\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # Placeholder for dynamic message content\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Dahab, it's nice to meet you! I'll remember your name. \\n\\nIs there anything I can help you with today? üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 29, 'total_tokens': 66, 'completion_time': 0.067272727, 'prompt_time': 0.00015918, 'queue_time': 0.021137617, 'total_time': 0.067431907}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-6e24ae6e-b33d-4b56-a373-4f6da49049eb-0', usage_metadata={'input_tokens': 29, 'output_tokens': 37, 'total_tokens': 66})"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Integration\n",
    "# Combine the prompt with the model to create a processing chain\n",
    "chain=prompt|model\n",
    "# Invoke the chain with initial user input\n",
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Dahab.\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Dahab, it's nice to meet you!  \\n\\nI'll remember your name. How can I help you today? üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 29, 'total_tokens': 63, 'completion_time': 0.061818182, 'prompt_time': 0.000141959, 'queue_time': 0.020891409, 'total_time': 0.061960141}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-fdf4396f-4e7f-4914-9cf5-273faae9e5a0-0', usage_metadata={'input_tokens': 29, 'output_tokens': 34, 'total_tokens': 63})"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enabling Memory with Runnable\n",
    "# Initialize a runnable with message history for managing session-specific interactions\n",
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)\n",
    "\n",
    "# Configuration for a new session with session ID 'chat3'\n",
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "\n",
    "# Invoking the model within a session to maintain context\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Dahab.\"),],\n",
    "    config=config\n",
    ")\n",
    "response # Display the model's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Enhancement with Complex Instructions\n",
    "# Import prompt components for enhanced templating\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Enhanced prompt setup with language customization\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all the questions to the best of your ability in {language}.\", # System instruction with variable language support\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # Dynamic message content\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡§æ‡§π‡§¨! ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡•Ç‡§õ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•Ç‡§Ç‡§ó‡§æ‡•§ üòä \\n\\n'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Advanced Model Chain and Interaction\n",
    "# Combine updated prompt with model to process language-specific requests\n",
    "chain=prompt|model\n",
    "\n",
    "# Invoke the chain with a language specification\n",
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Dahab\")],\"language\":\"Hindi\"})\n",
    "response.content # Display response content in specified language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap this additional chain complexity in a Message History class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Session Management\n",
    "# Enhance the runnable to handle complex input structures\n",
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\" # Specify the input key for message history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡§æ‡§π‡§¨! ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡•Ç‡§õ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•Ç‡§Ç‡§ó‡§æ‡•§ üòä \\n\\n'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration for a new session with session ID 'chat4'\n",
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "\n",
    "# Interactive response handling with language specificity\n",
    "respose=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi, I am Dahab\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response.content # Display AI response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§¶‡§æ‡§π‡§¨ ‡§π‡•à‡•§  \\n'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking AI's memory and understanding in the session\n",
    "response = with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"What's my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response.content  # Display AI response confirming user info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing the Conversation History\n",
    "To keep the messages within bound and context window. Limit the size of messages being parsed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a good assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is 2+2?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='No problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Message Trimming Setup\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "# Create a trimmer to control message size and inclusion criteria\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,  # Limit the message length to a maximum of 45 tokens\n",
    "    strategy=\"last\",  # Use the strategy to keep the last relevant messages within the limit\n",
    "    token_counter=model,  # Use model to count tokens\n",
    "    include_system=True,  # Include system messages in the trimming process\n",
    "    allow_partial=False,  # Do not allow partial messages\n",
    "    start_on=\"human\"  # Begin trimming from the first human message\n",
    ")\n",
    "\n",
    "# Define a series of initial messages in the conversation\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a good assistant\"),\n",
    "    HumanMessage(content=\"Hi! I'm Dan\"),\n",
    "    AIMessage(content=\"Hi!\"),\n",
    "    HumanMessage(content=\"I like Chicken Sandwiches\"),\n",
    "    AIMessage(content=\"That's nice\"),\n",
    "    HumanMessage(content=\"What is 2+2?\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"Thanks!\"),\n",
    "    AIMessage(content=\"No problem!\"),\n",
    "    HumanMessage(content=\"Having fun?\"),\n",
    "    AIMessage(content=\"Yes!\"),\n",
    "]\n",
    "\n",
    "# Apply the trimmer to the messages\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to your personal information or preferences, so I wouldn't know what kind of sandwich you like! ü•™\\n\\nWhat's your favorite sandwich? üòä  \\n\""
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain Creation and Initial Interaction\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define a processing chain that trims and passes messages\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer) # Chain to process and trim messages\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "# Invoke the chain with a query about preferences\n",
    "response=chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content=\"What sandwich do I like?\")], # Add user question to the message\n",
    "        \"language\": \"English\"\n",
    "    }\n",
    "\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"What is 2+2?\"  \\n\\n\\n\\nLet me know if you want to try another one! üòä \\n'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory Enhancement and Problem Inquiry\n",
    "# Invoke the chain with a follow-up question about previous interactions\n",
    "response=chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content=\"What match problem did I ask?\")], # User asks about previous math question\n",
    "        \"language\": \"English\"\n",
    "    }\n",
    "\n",
    ")\n",
    "response.content # Output AI response about previous questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Interactive Session \n",
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history, # Function to manage and retrieve session history\n",
    "    input_messages_key=\"messages\" # Specify the input key for message history\n",
    ")\n",
    "\n",
    "# Set the configuration for session tracking\n",
    "config = {\"configurable\": {\"session_id\": \"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations. If you'd like to ask me a math problem, I'm happy to help! üòä  \\n\""
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle interactive response with no session history - Question 1\n",
    "respose=with_message_history.invoke(\n",
    "    {\n",
    "        'messages': messages + [HumanMessage(content=\"What is my name?\")], # User asks a follow-up question\n",
    "        \"language\": \"English\" # Maintain English context\n",
    "    },\n",
    "    config=config, # Apply session configuration\n",
    ")\n",
    "response.content # Display AI response based on session memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations. If you'd like to ask me a math problem, I'm happy to help! üòä  \\n\""
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle interactive response with no session history - Question 2\n",
    "respose=with_message_history.invoke(\n",
    "    {\n",
    "        'messages': [HumanMessage(content=\"What math problem did I ask?\")], # User asks a follow-up question\n",
    "        \"language\": \"English\" # Maintain English context\n",
    "    },\n",
    "    config=config, # Apply session configuration\n",
    ")\n",
    "response.content # Display AI response based on session memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91146359afa204c864d2e8c089978309c47dcbcfea673fcbf773823672987b6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
